{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da83c5a-4991-42e0-8244-1ff315ce6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdcac31-11cd-48dc-b39a-8319237145cd",
   "metadata": {},
   "source": [
    "## Define Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a490f57-21b1-4c8b-83b3-97583e9c3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\",\n",
    "                 \"dog cat animal\", \"cat animal dog\", \"cat dog animal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff67272-3337-49dc-b518-1b182968de12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'animal', 'dog'],\n",
       " ['cat', 'dog', 'animal']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [sent.split(\" \") for sent in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43350b21-e414-4582-81c0-66dc15f1b55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'cat', 'apple', 'animal', 'banana', 'dog']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(corpus)))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60101ca1-15ae-43c0-903c-2c329c2d64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fruit': 0, 'cat': 1, 'apple': 2, 'animal': 3, 'banana': 4, 'dog': 5}\n"
     ]
    }
   ],
   "source": [
    "#appending number for each word in the set\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6692ae17-17ca-41be-9204-b7034282b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#vocab size\n",
    "voc_size = len(vocab)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e30e530-5097-4a34-b738-12b7db634316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append UNK\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8469afb-2f61-4be5-8134-080a1e4536fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'cat', 'apple', 'animal', 'banana', 'dog', '<UNK>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76cb903-2b90-4d08-96f4-5e773faa2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a method to get the word through index\n",
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efe6e8-715a-4323-8e54-cbbc5668d89b",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceda5be8-102e-4e7c-ac14-79ffdd0b52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence):\n",
    "    \n",
    "    # Make skip gram of one size window\n",
    "    skip_grams = []\n",
    "    for sent in corpus:\n",
    "        for i in range(1, len(sent) - 1):\n",
    "            target = word2index[sent[i]]\n",
    "            context = [word2index[sent[i - 1]], word2index[sent[i + 1]]]\n",
    "            for w in context:\n",
    "                skip_grams.append([target, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bcbed0f-c099-4c10-a63b-1afb61c1107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[1]\n",
      " [0]]\n",
      "Target:  [[3]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch = random_batch(batch_size, corpus)\n",
    "\n",
    "print(\"Input: \",  input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a89c2ca4-fd84-487c-9527-da3874c507d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1), (2, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.shape, target_batch.shape"
   ]
  },
  {
   "attachments": {
    "d232a585-8f82-4deb-b9ba-e3e7fc16980c.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c10fac73-5fa8-4407-848e-7fd4a95e6bbf",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "![image.png](attachment:d232a585-8f82-4deb-b9ba-e3e7fc16980c.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f43aacf-af4c-4a44-a7ac-1ee98ee0f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411131de-7f72-491d-bcc2-1dfa7913ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum([c for w, c in word_count.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae03ff5-4f3d-4936-85eb-644a41db5238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count[',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea41b5d-93ac-4662-a85d-e4d0a8ca43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocab:\n",
    "    unigram_table.extend([vo] * int(((word_count[vo]/num_total_words)**0.75)/Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc0e7c8-3891-4df4-8b59-cc3d0cfc190a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'fruit': 260,\n",
       "         'cat': 260,\n",
       "         'apple': 260,\n",
       "         'animal': 260,\n",
       "         'banana': 260,\n",
       "         'dog': 260})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4554d4ae-328a-48cc-be23-12a53a2cd262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k: # num of sampling\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b39a18f-025a-428c-8403-2e934628a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch  = torch.Tensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5880b62-a67e-456d-9e64-3da2aa6c8b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee29d63b-1f82-480d-8d85-64e9369e324a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 0, 0],\n",
       "        [5, 3, 5]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_neg = 3\n",
    "negative_sampling(target_batch, unigram_table, num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47864f9b-56a9-4689-8431-d9d319463f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a8619-223a-4943-952f-b1eed9eb302d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aee61882-37e7-4062-8802-caf3e92fb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size) # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds    = -self.embedding_u(negative_words) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, k, 1]\n",
    "        \n",
    "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
    "                \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc78005-4f3e-4015-b300-17a0ce43f23f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2238234-dfa0-4b35-92ff-73042f09a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "model          = SkipgramNegSampling(voc_size, embedding_size)\n",
    "num_neg        = 10 # num of negative sampling\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Environment",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
